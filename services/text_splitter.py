# services/text_splitter.py

import tiktoken
from utils.count_tokens import count_tokens
from utils.logger import Logger
from utils.config import Config
from utils.dependency_injector import inject_dependencies

@inject_dependencies
def split_transcript_by_tokens(
    text, 
    max_tokens=None, 
    model=None,
    logger: Logger = None,
    config: Config = None
):
    """
    Divise un texte en chunks qui ne d√©passent pas le nombre maximal de tokens.
    
    Args:
        text: Le texte √† diviser
        max_tokens: Nombre maximal de tokens par chunk (d√©faut: config.MAX_TOKENS)
        model: Mod√®le √† utiliser pour l'encodage (d√©faut: config.OPENAI_ENCODING_MODEL)
        logger: Instance de logger inject√©e
        config: Instance de configuration inject√©e
        
    Returns:
        list: Liste de chunks de texte
    """
    max_tokens = max_tokens or config.MAX_TOKENS
    model = model or config.OPENAI_ENCODING_MODEL
    
    if not isinstance(text, str) or not text.strip():
        logger.warning("‚ö†Ô∏è Tentative de division d'un texte vide ou invalide")
        return []

    logger.debug(f"üìè Fractionnement du texte en chunks (max tokens: {max_tokens}, mod√®le: {model})")
    
    try:
        encoding = tiktoken.encoding_for_model(model)
        sentences = text.split('.')  # peut √™tre am√©lior√© plus tard
        chunks = []
        current_chunk = ""
        
        total_sentences = len(sentences)
        logger.debug(f"üîç {total_sentences} phrases √† traiter")

        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue

            sentence_with_dot = sentence + "."
            sentence_tokens = len(encoding.encode(sentence_with_dot))

            # üö® Cas limite : phrase trop longue √† elle seule
            if sentence_tokens > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                chunks.append(sentence_with_dot.strip())
                logger.debug(f"‚ö†Ô∏è Phrase {i+1}/{total_sentences} trop longue ({sentence_tokens} tokens), ajout√©e comme chunk s√©par√©")
                continue

            # ‚úÖ V√©rifier si le chunk + la phrase tiennent dans la limite
            temp_chunk = current_chunk + sentence_with_dot
            temp_token_count = len(encoding.encode(temp_chunk))

            if temp_token_count <= max_tokens:
                current_chunk = temp_chunk
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sentence_with_dot

        if current_chunk:
            chunks.append(current_chunk.strip())

        logger.debug(f"‚úÖ Texte divis√© en {len(chunks)} chunks")
        return chunks
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la division du texte: {str(e)}")
        # En cas d'erreur, retourner le texte entier dans un seul chunk
        return [text]