from services.ollama_client import call_ollama_llm
from services.openai_client import call_openai_llm
from utils.decorators import safe_exec, log_execution, timed
from utils.logger import get_logger
import os
from dotenv import load_dotenv

logger = get_logger()
load_dotenv()

@timed
@log_execution
@safe_exec
def get_llm_client(engine, api_url=None, api_key=None):
    """
    Retourne la fonction IA appropri√©e selon le choix utilisateur.
    - Si openai-user est s√©lectionn√© et cl√© + URL pr√©sentes ‚Üí OpenAI (cl√© utilisateur)
    - Si openai-default est s√©lectionn√© ‚Üí OpenAI avec cl√© du projet
    - Sinon, fallback Ollama
    """
    logger.info(f"üîÑ Configuration du client LLM avec moteur: {engine}")
    
    if engine == "ollama":
        logger.info("‚úì Utilisation du client Ollama local")
        return call_ollama_llm
    
    elif engine == "openai-user":
        # V√©rifier mais ne pas logger la cl√© API
        if not api_key:
            logger.warning("‚ö†Ô∏è Aucune cl√© API fournie pour OpenAI-User")
        else:
            logger.info("‚úì Cl√© API utilisateur fournie pour OpenAI")
            
        # Masquer la cl√© dans les logs
        if api_url:
            logger.info(f"‚úì URL API personnalis√©e: {api_url}")
        
        # Retourner le client avec les param√®tres
        return lambda prompt: call_openai_llm(prompt, api_key=api_key, api_base=api_url)
    
    else:  # openai-default
        # Utiliser la cl√© par d√©faut de l'environnement
        default_api_key = os.getenv("OPENAI_API_KEY")
        if not default_api_key:
            logger.warning("‚ö†Ô∏è Aucune cl√© API par d√©faut dans les variables d'environnement")
        else:
            logger.info("‚úì Utilisation de la cl√© API OpenAI par d√©faut")
            
        # Retourner le client avec la cl√© par d√©faut
    return lambda prompt: call_openai_llm(prompt)