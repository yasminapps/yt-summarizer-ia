from services.ollama_client import call_ollama_llm
from services.openai_client import call_openai_llm
from utils.decorators import safe_exec, log_execution, timed
from utils.dependency_injector import inject_dependencies
from utils.logger import Logger
from utils.config import Config
import os
from dotenv import load_dotenv

load_dotenv()

@timed
@log_execution
@safe_exec
@inject_dependencies
def get_llm_client(
    engine: str, 
    api_url: str = None, 
    api_key: str = None,
    logger: Logger = None,
    config: Config = None
):
    """
    Retourne la fonction IA appropri√©e selon le choix utilisateur.
    
    Args:
        engine: Moteur IA √† utiliser (ollama, openai-user, openai-default)
        api_url: URL de l'API (pour openai-user)
        api_key: Cl√© API (pour openai-user)
        logger: Instance de logger inject√©e
        config: Instance de configuration inject√©e
        
    Returns:
        Callable: Fonction qui prend un prompt et retourne une r√©ponse
    """
    logger.info(f"üîÑ Configuration du client LLM avec moteur: {engine}")
    
    # V√©rifier que le moteur est autoris√©
    if engine not in config.ALLOWED_ENGINES:
        logger.warning(f"‚ö†Ô∏è Moteur {engine} non reconnu, utilisation du moteur par d√©faut")
        engine = "openai-default"
    
    if engine == "ollama":
        logger.info("‚úì Utilisation du client Ollama local")
        return lambda prompt: call_ollama_llm(prompt, logger=logger, config=config)
    
    elif engine == "openai-user":
        # V√©rifier mais ne pas logger la cl√© API
        if not api_key:
            logger.warning("‚ö†Ô∏è Aucune cl√© API fournie pour OpenAI-User")
        else:
            logger.info("‚úì Cl√© API utilisateur fournie pour OpenAI")
            
        # Masquer la cl√© dans les logs
        if api_url:
            logger.info(f"‚úì URL API personnalis√©e: {api_url}")
        
        # Retourner le client avec les param√®tres
        return lambda prompt: call_openai_llm(
            prompt, 
            api_key=api_key, 
            api_url=api_url,
            model=config.OPENAI_MODEL,
            logger=logger,
            config=config
        )
    
    else:  # openai-default
        # Utiliser la cl√© par d√©faut de l'environnement
        default_api_key = os.getenv("OPENAI_API_KEY")
        if not default_api_key:
            logger.warning("‚ö†Ô∏è Aucune cl√© API par d√©faut dans les variables d'environnement")
        else:
            logger.info("‚úì Utilisation de la cl√© API OpenAI par d√©faut")
            
        # Retourner le client avec la cl√© par d√©faut et les d√©pendances inject√©es
        return lambda prompt: call_openai_llm(
            prompt,
            model=config.OPENAI_MODEL,
            logger=logger,
            config=config
        )