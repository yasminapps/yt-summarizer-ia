# ğŸ—ï¸ Project Architecture â€” YouTube Transcript & Summary Generator

This document outlines the complete technical architecture of the project, including its backend/frontend structure, AI engine integration, token-based text chunking system, dependency injection mechanism, and test strategy.

---

## âš™ï¸ Overview

The application extracts transcripts from YouTube videos and uses an AI engine (OpenAI or Ollama) to generate structured summaries with customizable formats and styles.

---

## ğŸ§± Application Structure

```bash
.
â”œâ”€â”€ app.py                   # Flask entrypoint
â”œâ”€â”€ routes/                 # Flask routes (summary, transcript)
â”œâ”€â”€ services/               # Business logic (AI clients, transcript, prompt, splitters)
â”œâ”€â”€ utils/                  # Shared helpers (config, logger, decorators, sanitizer)
â”œâ”€â”€ templates/              # HTML (Jinja2 templates)
â”œâ”€â”€ static/                 # JS, CSS, favicon
â”œâ”€â”€ prompts/                # Prompt templates (externalized)
â”œâ”€â”€ tests/                  # Unit + integration tests (Pytest + Jest)
â”œâ”€â”€ requirements.txt
â””â”€â”€ run.sh
```

---

## ğŸ§  AI Engine Integration

Supported AI engines:
- `OpenAI` (GPT-4o by default via API)
- `Ollama` (local models via local endpoint)

Users can:
- Use the platformâ€™s default OpenAI key
- Provide their own OpenAI API key + endpoint

Selection is handled via a radio input and passed to the backend with each request.

---

## ğŸª„ Summary Generation Flow

1. **User submits a YouTube URL + customization options**
2. **Transcript extraction** via YouTube API
3. **Transcript is split into token-safe chunks** using `tiktoken`
4. First chunk â†’ `build_initial_prompt()`  
   Other chunks â†’ `build_update_prompt(previous_summary)`
5. Each chunk is sent to the LLM client (OpenAI/Ollama)
6. Each partial summary is chained with the previous one
7. Final result is converted to HTML using `markdown.markdown()` and returned

---

## âœ‚ï¸ Token-Based Text Splitting

- Transcript is split by sentence.
- Each chunk is encoded with `tiktoken` and must remain under `MAX_TOKENS`.
- If a single sentence exceeds the limit, it is isolated into its own chunk.

```python
split_transcript_by_tokens(text, max_tokens=10000, model="gpt-3.5-turbo")
```

---

## ğŸ§© Dependency Injection System

Decorators used:
- `@inject_logger`
- `@inject_config`
- `@inject_dependencies` (for both)

This allows services and routes to receive pre-injected `config` and `logger` objects. No global import of `config` or `get_logger()` in logic functions = more modular and testable.

---

## ğŸ§ª Testing Strategy

### Backend (Python â€“ Pytest)

- Unit tests for all services: `youtube_transcript`, `ollama_client`, `openai_client`, `splitter`, `prompt_builder`
- Integration tests for Flask routes: `summarize`, `get_transcript_only`

### Frontend (JavaScript â€“ Jest + jsdom)

- DOM logic (copy, download, loading animation, clipboard)
- Uses `@jest-environment jsdom` to simulate browser environment
- HTML template is loaded dynamically before each test

```bash
cd tests/test_frontend
npm test
```

---

## ğŸ” CI/CD

CI is powered by **GitHub Actions**:
- Python tests (pytest) and JS tests (Jest) run on each push and pull request
- Node.js and Python environments are both set up and cached
- Workflow: `.github/workflows/test.yml`

---

## ğŸ³ Docker (Coming soon)

A `Dockerfile` and `docker-compose.yml` are planned to encapsulate:
- Flask server
- Ollama service
- Node.js build for JS tests (optional)

---

## ğŸ§© Modularity

- Prompt logic is externalized in `/prompts/`
- Config is isolated and validated using `pydantic-settings`
- Logger is injectable for mocking
- Backend is test-friendly and ready for scaling (even without microservices)

---

## âœ… Notes

- App is local-first, with future deployment to Hostinger VPS
- User API key support and language autodetection are in production
- No database, no persistent storage: pure stateless generation